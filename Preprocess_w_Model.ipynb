{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28ed303",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "import nltk\n",
    "from nltk import word_tokenize, FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "import lightgbm as lgb\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from keras.utils import pad_sequences\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6b6f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the dataset\n",
    "pt = pd.read_csv(\"./Dataset1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e3c7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = 0\n",
    "neg = 0\n",
    "for v in pt[\"Sentiment\"]:\n",
    "    if(v >= 0):\n",
    "        pos = pos + 1\n",
    "    else:\n",
    "        neg = neg + 1\n",
    "        \n",
    "print(\"Depressive : \",neg)\n",
    "print(\"Non-Depressive : \",pos)\n",
    "print(\"( S-pos : S-neg ) : (1:\", neg/pos,\")\")\n",
    "\n",
    "# 1 : depressed\n",
    "# 0 : depressed\n",
    "\n",
    "def diagnose(x):\n",
    "    if(x <= 0):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "pt[\"Diagnose\"] = pt[\"Sentiment\"].apply(lambda x: diagnose(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1b1d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this implements the mechanism to recorrect words to their correct most probable value\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "WORDS = Counter(words(open('big.txt').read()))\n",
    "\n",
    "def P(word, N=sum(WORDS.values())): \n",
    "    \"Probability of `word`.\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5fdb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = {}\n",
    "\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "w_tokenizer = TweetTokenizer()\n",
    "\n",
    "#tokenizes the tweet and corrects the words to their most probable similar form\n",
    "def tokenize_text(text):\n",
    "    tokens = w_tokenizer.tokenize((text))\n",
    "    tok_ar = []\n",
    "    for element in tokens:\n",
    "        corrected_word = correction(element)\n",
    "        tok_ar.append(corrected_word)\n",
    "        if element in count:\n",
    "            count[corrected_word] = count[corrected_word] + 1\n",
    "        else:\n",
    "             count[corrected_word] = 1;\n",
    "    return tok_ar\n",
    "\n",
    "#lemmatizes the tokenized entries from a tweet ti their original form\n",
    "def lemmatize_text(text):\n",
    "    return [(lemmatizer.lemmatize(w)) for w in text]\n",
    "\n",
    "#removing punctutations like ., , , ? etc.\n",
    "def remove_punctuation(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', (word))\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def preprocess_data(data):\n",
    "    words = data.apply(lemmatize_text)\n",
    "    words = words.apply(remove_punctuation)\n",
    "    return pd.DataFrame(words)\n",
    "\n",
    "#removing numbers if present from tweets\n",
    "pt['Tweet'] = pt['Tweet'].astype(str).apply(lambda x: re.sub('\\d+', '', x))\n",
    "lower_text = pt['Tweet'].str.lower()\n",
    "\n",
    "#calling tokenization on tweets\n",
    "pt['Tweet'] = pt['Tweet'].apply(tokenize_text)\n",
    "\n",
    "#stop-worlds like is are was, removed from the tweets\n",
    "stop_words = get_stop_words('english')\n",
    "pt['Tweet'] = pt['Tweet'].apply(lambda x: [item for item in x if item not in stop_words] )\n",
    "\n",
    "pre_tweets = preprocess_data(pt['Tweet'])\n",
    "pt['Tweet'] = pre_tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841a85e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# time of tweet to timesine and timecos\n",
    "pt['TimeSin'] = np.sin(2 * np.pi * pd.to_datetime(pt['Time'],format='%H:%M:%S').dt.hour / 24)\n",
    "pt['TimeCos'] = np.cos(2 * np.pi * pd.to_datetime(pt['Time'],format='%H:%M:%S').dt.hour / 24)\n",
    "\n",
    "# encoding location to a numerical data\n",
    "le = LabelEncoder()\n",
    "pt[\"location\"]=le.fit_transform(pt[\"location\"])\n",
    "print(pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7373dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rejoin the tokens\n",
    "def joinop(x):\n",
    "    s = \"\"\n",
    "    for i in x:\n",
    "        s = s + i + \" \";\n",
    "    print(s)\n",
    "    return s\n",
    "pt['Tweet'] = pt['Tweet'].apply(lambda x:  joinop(x))\n",
    "print(pt['Tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45faedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare for att-bilstm\n",
    "\n",
    "X = pt.iloc[:,4].copy()\n",
    "target_label = pt.iloc[:,9]\n",
    "\n",
    "voc_size= len(count) + 1\n",
    "onehot_repr=[one_hot(words,voc_size)for words in X] \n",
    "onehot_repr\n",
    "\n",
    "sent_length = 0\n",
    "for x in onehot_repr:\n",
    "    if sent_length < len(x):\n",
    "        sent_length = len(x)\n",
    "\n",
    "embedded_docs=pad_sequences(onehot_repr,padding='pre',maxlen=sent_length)\n",
    "\n",
    "X_l=np.array(embedded_docs)\n",
    "y_l=np.array(target_label)\n",
    "\n",
    "# train test split for A_BiLSTM\n",
    "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X_l, y_l, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f9b973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention layer \n",
    "\n",
    "class attention(Layer):\n",
    "    def __init__(self, return_sequences=True):\n",
    "        self.return_sequences = return_sequences\n",
    "\n",
    "        super(attention,self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W=self.add_weight(name=\"att_weight\", shape=(input_shape[-1],1), initializer=\"normal\")\n",
    "        self.b=self.add_weight(name=\"att_bias\", shape=(input_shape[1],1),initializer=\"normal\")\n",
    "        super(attention,self).build(input_shape)\n",
    "\n",
    "\n",
    "    def call(self, x):\n",
    "        e = K.tanh(K.dot(x,self.W)+self.b)\n",
    "        a = K.softmax(e, axis=1)\n",
    "        output = x*a\n",
    "        if self.return_sequences:\n",
    "            return output\n",
    "        return K.sum(output, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e4c829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model1 : attention bi-lstm\n",
    "\n",
    "model1 = Sequential()\n",
    "model1.add(Embedding(voc_size, 128, input_length=sent_length))\n",
    "model1.add(Bidirectional(LSTM(100, return_sequences=True)))\n",
    "model1.add(attention(return_sequences=False))\n",
    "model1.add(Dense(1, activation='sigmoid'))\n",
    "model1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b40fc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model1 : attention bi-lstm training\n",
    "model1.fit(X_train_1,y_train_1,validation_data=(X_test_1,y_test_1),epochs=6,batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ea0be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model2 : naive bias\n",
    "\n",
    "tfidf = TfidfVectorizer(sublinear_tf = True, max_df = 0.5,min_df = 0.001, stop_words='english',ngram_range = (1,2))\n",
    "tf_X = tfidf.fit_transform(X)\n",
    "tf_X = tf_X.toarray()\n",
    "tf_y = np.array(target_label)\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(tf_X, tf_y, test_size=0.2, random_state=42)\n",
    "model2 = MultinomialNB()\n",
    "\n",
    "# model2 : naive bias training\n",
    "model2.fit(X_train_2,y_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874f9f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model3 : xgboost\n",
    "location_feature_array = pt.iloc[:,5].values.reshape(-1, 1)\n",
    "time_feature_array_sin = pt.iloc[:,10].values.reshape(-1, 1)\n",
    "time_feature_array_cos = pt.iloc[:,11].values.reshape(-1, 1)\n",
    "text_feature_dense = tf_X\n",
    "\n",
    "all_features = np.concatenate((text_feature_dense, location_feature_array, time_feature_array_sin, time_feature_array_cos), axis=1)\n",
    "X_train_3, X_test_3, y_train_3, y_test_3 = train_test_split(all_features, tf_y, test_size=0.2, random_state=42)\n",
    "model3 = XGBClassifier(use_label_encoder=False, \n",
    "                      booster='gbtree', # boosting algorithm to use, default gbtree, othera: gblinear, dart\n",
    "                      n_estimators=100, # number of trees, default = 100\n",
    "                      eta=0.3, # this is learning rate, default = 0.3\n",
    "                      max_depth=6, # maximum depth of the tree, default = 6\n",
    "                      gamma = 0, # used for pruning, if gain < gamma the branch will be pruned, default = 0\n",
    "                      reg_lambda = 1, # regularization parameter, defautl = 1\n",
    "                      #min_child_weight=0 # this refers to Cover which is also responsible for pruning if not set to 0\n",
    "                     )\n",
    "\n",
    "#training\n",
    "model3.fit(X_train_3, y_train_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a72edaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model4 : lightgbm\n",
    "\n",
    "model4 = lgb.LGBMClassifier()\n",
    "model4.fit(X_train_3, y_train_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcc1860",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model5 : random forest\n",
    "\n",
    "model5 = RandomForestClassifier()#(n_estimators = 1000, random_state = 42)\n",
    "model5.fit(X_train_3, y_train_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daba6055",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model6 : linear SVC classifier\n",
    "\n",
    "model6 = LinearSVC(max_iter=100000,random_state=123)\n",
    "model6.fit(X_train_3, y_train_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7995392d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction model 1\n",
    "\n",
    "temp_pred1 = model1.predict(X_test_1)\n",
    "threshold = 0.5\n",
    "pred1 = (temp_pred1 > threshold).astype(int)\n",
    "\n",
    "# prediction model 2\n",
    "pred2 = model2.predict(X_test_2)\n",
    "\n",
    "# prediction model 3\n",
    "pred3 = model3.predict(X_test_3)\n",
    "\n",
    "# prediction model 3\n",
    "pred4 = model4.predict(X_test_3)\n",
    "\n",
    "# prediction model 3\n",
    "pred5 = model5.predict(X_test_3)\n",
    "\n",
    "# prediction model 3\n",
    "pred6 = model6.predict(X_test_3)\n",
    "\n",
    "print(\"Accuracy : lstm : \",metrics.accuracy_score(y_test_1, pred1))\n",
    "print(\"Accuracy : naive : \",metrics.accuracy_score(y_test_2, pred2))\n",
    "print(\"Accuracy : xgboost : \",metrics.accuracy_score(y_test_3, pred3))\n",
    "print(\"Accuracy : lightgbm : \",metrics.accuracy_score(y_test_3, pred4))\n",
    "print(\"Accuracy : random forest : \",metrics.accuracy_score(y_test_3, pred5))\n",
    "print(\"Accuracy : linear SVC : \",metrics.accuracy_score(y_test_3, pred6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9c732b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "y_actual = y_test_1\n",
    "elements = [pred1, pred2, pred3, pred4, pred5,pred6]\n",
    "\n",
    "def func(a,index):\n",
    "    if(a == 0):\n",
    "        return -1*W[index]\n",
    "    else:\n",
    "        return 1*W[index]\n",
    "    \n",
    "names = [f\"pred{i}\" for i in range(1, len(elements) + 1)]\n",
    "res = {}\n",
    "\n",
    "w_final = []\n",
    "\n",
    "for i in range(1, len(elements) + 1):\n",
    "    # Generate all combinations of length i\n",
    "    for indices in itertools.combinations(range(len(elements)), i):\n",
    "        \n",
    "        combination_names = tuple(names[index] for index in indices)\n",
    "        combination = [elements[index] for index in indices]\n",
    "        \n",
    "        y_preds = combination\n",
    "        D = []\n",
    "        for pred in y_preds:\n",
    "            ct = 0\n",
    "            for obd,act in zip(pred,y_actual):\n",
    "                if(obd != act):\n",
    "                    ct = ct + 1\n",
    "            D.append(ct)\n",
    "\n",
    "        denom = 0\n",
    "        W = []\n",
    "\n",
    "        for Dk in D:\n",
    "            denom = denom + 1/Dk\n",
    "\n",
    "        for Dk in D:\n",
    "            weight = (1/Dk)/denom\n",
    "            W.append(weight)\n",
    "        if len(combination_names) == 3 and combination_names[0] == 'pred1' and combination_names[1] == 'pred5' and combination_names[2] == 'pred6':\n",
    "            w_final = W #model weights for max accuracy\n",
    "        y_pred = []\n",
    "        for row in zip(*iter(y_preds)):\n",
    "            pred = 0\n",
    "            for index,value in enumerate(row):\n",
    "                pred = pred + func(value,index)\n",
    "\n",
    "            if(pred >=0):\n",
    "                y_pred.append(1)\n",
    "            else:\n",
    "                y_pred.append(0)\n",
    "        \n",
    "        res[combination_names] = metrics.accuracy_score(y_actual, y_pred)\n",
    "\n",
    "sortens = sorted(res.items(), key=lambda x:x[1], reverse=True)\n",
    "converted_dict = dict(sortens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5727582",
   "metadata": {},
   "outputs": [],
   "source": [
    "#all the combinations of model and ensemble accuracy\n",
    "\n",
    "for item in converted_dict.items():\n",
    "    print(item,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2011fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset 2 for predicting (unlabelled dataset)\n",
    "\n",
    "gt = pd.read_csv('./Dataset2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0c0cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final chosen weights\n",
    "w_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3eb803",
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing the new datatset 2\n",
    "\n",
    "ct = {}\n",
    "\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "w_tokenizer = TweetTokenizer()\n",
    "\n",
    "#tokenizes the tweet and corrects the words to their most probable similar form\n",
    "def tokenize_text(text):\n",
    "    tokens = w_tokenizer.tokenize((text))\n",
    "    tok_ar = []\n",
    "    for element in tokens:\n",
    "        corrected_word = correction(element)\n",
    "        tok_ar.append(corrected_word)\n",
    "        if element in count:\n",
    "            ct[corrected_word] = count[corrected_word] + 1\n",
    "        else:\n",
    "             ct[corrected_word] = 1;\n",
    "    return tok_ar\n",
    "\n",
    "#lemmatizes the tokenized entries from a tweet ti their original form\n",
    "def lemmatize_text(text):\n",
    "    return [(lemmatizer.lemmatize(w)) for w in text]\n",
    "\n",
    "#removing punctutations like ., , , ? etc.\n",
    "def remove_punctuation(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', (word))\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def preprocess_data(data):\n",
    "    words = data.apply(lemmatize_text)\n",
    "    words = words.apply(remove_punctuation)\n",
    "    return pd.DataFrame(words)\n",
    "\n",
    "#removing numbers if present from tweets\n",
    "gt['Tweet'] = gt['Tweet'].astype(str).apply(lambda x: re.sub('\\d+', '', x))\n",
    "lower_text = gt['Tweet'].str.lower()\n",
    "\n",
    "#calling tokenization on tweets\n",
    "gt['Tweet'] = gt['Tweet'].apply(tokenize_text)\n",
    "\n",
    "#stop-worlds like is are was, removed from the tweets\n",
    "stop_words = get_stop_words('english')\n",
    "gt['Tweet'] = gt['Tweet'].apply(lambda x: [item for item in x if item not in stop_words] )\n",
    "\n",
    "pre_tweets = preprocess_data(gt['Tweet'])\n",
    "gt['Tweet'] = pre_tweets\n",
    "\n",
    "gt['TimeSin'] = np.sin(2 * np.pi * pd.to_datetime(gt['Time'],format='%H:%M:%S').dt.hour / 24)\n",
    "gt['TimeCos'] = np.cos(2 * np.pi * pd.to_datetime(gt['Time'],format='%H:%M:%S').dt.hour / 24)\n",
    "\n",
    "gt[\"location\"]=le.transform(gt[\"location\"])\n",
    "def joinop(x):\n",
    "    s = \"\"\n",
    "    for i in x:\n",
    "        s = s + i + \" \";\n",
    "    print(s)\n",
    "    return s\n",
    "gt['Tweet'] = gt['Tweet'].apply(lambda x:  joinop(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a3b831",
   "metadata": {},
   "outputs": [],
   "source": [
    "rx = gt.iloc[:,4].copy()\n",
    "\n",
    "voc_size= len(ct) + 1\n",
    "oh_rep=[one_hot(words,voc_size)for words in rx] \n",
    "oh_rep\n",
    "\n",
    "sent_length = 0\n",
    "for x in oh_rep:\n",
    "    if sent_length < len(x):\n",
    "        sent_length = len(x)\n",
    "\n",
    "embedded_docs=pad_sequences(oh_rep,padding='pre',maxlen=361)\n",
    "\n",
    "res_1 = np.array(embedded_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609c2d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Conc_res = []\n",
    "for a,b,c in zip(p1,p2,p3):\n",
    "    sum  = 0\n",
    "    if a == 0:\n",
    "        sum = sum + -1*w_final[0]\n",
    "    else:\n",
    "        sum = sum + 1*w_final[0]\n",
    "    if b == 0:\n",
    "        sum = sum + -1*w_final[1]\n",
    "    else:\n",
    "        sum = sum + 1*w_final[1]\n",
    "    if c == 0:\n",
    "        sum = sum + -1*w_final[2]\n",
    "    else:\n",
    "        sum = sum + 1*w_final[2]\n",
    "        \n",
    "    if sum >= 0:\n",
    "        Conc_res.append(1)\n",
    "    else:\n",
    "        Conc_res.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1d5654",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalpush = pd.read_csv('./Dataset2.csv')\n",
    "finalpush[\"Modelled Dep. Status\"] = Conc_res\n",
    "print(finalpush)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69226282",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalpush.to_csv('FinalResult.csv', sep=',', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
