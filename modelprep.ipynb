{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e28ed303",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "import nltk\n",
    "from nltk import word_tokenize, FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d6b6f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the dataset\n",
    "dt = pd.read_excel(\"./LABELDATA.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "254212d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_locations = ['south africa', 'nigeria', 'canada', 'philippines', 'united states', 'ireland', 'japan',\n",
    "                   'pakistan', 'australia', 'india', 'kenya', 'united kingdom', 'mali', 'puerto rico']\n",
    "\n",
    "# Filter rows based on the valid locations\n",
    "dt = dt[dt['location'].isin(valid_locations)]\n",
    "\n",
    "# Reset the index of the DataFrame\n",
    "dt.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92e3c7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depressive :  17503\n",
      "Non-Depressive :  25155\n",
      "( S-pos : S-neg ) : (1: 0.695806002782747 )\n"
     ]
    }
   ],
   "source": [
    "pos = 0\n",
    "neg = 0\n",
    "for v in dt[\"Sentiment\"]:\n",
    "    if(v >= 0):\n",
    "        pos = pos + 1\n",
    "    else:\n",
    "        neg = neg + 1\n",
    "        \n",
    "print(\"Depressive : \",neg)\n",
    "print(\"Non-Depressive : \",pos)\n",
    "print(\"( S-pos : S-neg ) : (1:\", neg/pos,\")\")\n",
    "\n",
    "def diagnose(x):\n",
    "    if(x <= 0):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "dt[\"Diagnose\"] = dt[\"Sentiment\"].apply(lambda x: diagnose(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "488c67fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     I can't even express how angry I was when I fo...\n",
      "1     Sadness and Anger for the ones who didn't vote...\n",
      "2     Vale, Geoffrey Love. We would like to share ou...\n",
      "3     Hahaha story if my life and sadness of my sex ...\n",
      "4     I've never heard of your store before but now ...\n",
      "                            ...                        \n",
      "95    nah.just pitch black for eternity with a side ...\n",
      "96    Really wanna explore the feeling of loneliness...\n",
      "97    Hello . heres a podcast I thought you would li...\n",
      "98    Aged or , anyone can volunteer. There are mill...\n",
      "99    Maybe you can reach out to someone you love + ...\n",
      "Name: Tweet, Length: 100, dtype: object\n"
     ]
    }
   ],
   "source": [
    "pt = dt.iloc[:100,:].copy()\n",
    "print(pt[\"Tweet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b302b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this implements the mechanism to recorrect words to their correct most probable value\n",
    "\n",
    "\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "WORDS = Counter(words(open('big.txt').read()))\n",
    "\n",
    "def P(word, N=sum(WORDS.values())): \n",
    "    \"Probability of `word`.\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c5fdb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "w_tokenizer = TweetTokenizer()\n",
    "\n",
    "#tokenizes the tweet and corrects the words to their most probable similar form\n",
    "def tokenize_text(text):\n",
    "    tokens = w_tokenizer.tokenize((text))\n",
    "    tok_ar = []\n",
    "    for element in tokens:\n",
    "        tok_ar.append(correction(element))\n",
    "    return tok_ar\n",
    "\n",
    "#lemmatizes the tokenized entries from a tweet ti their original form\n",
    "def lemmatize_text(text):\n",
    "    return [(lemmatizer.lemmatize(w)) for w in text]\n",
    "\n",
    "#removing punctutations like ., , , ? etc.\n",
    "def remove_punctuation(words):\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', (word))\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def preprocess_data(data):\n",
    "    words = data.apply(lemmatize_text)\n",
    "    words = words.apply(remove_punctuation)\n",
    "    return pd.DataFrame(words)\n",
    "\n",
    "#removing numbers if present from tweets\n",
    "pt['Tweet'] = pt['Tweet'].astype(str).apply(lambda x: re.sub('\\d+', '', x))\n",
    "lower_text = pt['Tweet'].str.lower()\n",
    "\n",
    "#calling tokenization on tweets\n",
    "pt['Tweet'] = pt['Tweet'].apply(tokenize_text)\n",
    "\n",
    "#stop-worlds like is are was, removed from the tweets\n",
    "stop_words = get_stop_words('english')\n",
    "pt['Tweet'] = pt['Tweet'].apply(lambda x: [item for item in x if item not in stop_words] )\n",
    "\n",
    "pre_tweets = preprocess_data(pt['Tweet'])\n",
    "pt['Tweet'] = pre_tweets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58859fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               User      Time Language             Tweet ID  \\\n",
      "0   MotherO12536172  10:10:59       en  1661310000000000000   \n",
      "1      Soulseekerk9  08:07:53       en  1661280000000000000   \n",
      "2   HousingFirstLtd  07:51:18       en  1661280000000000000   \n",
      "3         engin_no9  07:09:59       en  1661270000000000000   \n",
      "4         SmartIsnt  07:03:27       en  1661270000000000000   \n",
      "..              ...       ...      ...                  ...   \n",
      "95     TamatoaPride  04:11:33       en  1660140000000000000   \n",
      "96          Carleey  23:46:20       en  1660070000000000000   \n",
      "97     VitoCarrozzo  21:41:19       en  1660040000000000000   \n",
      "98      HelloCareAU  21:30:00       en  1660040000000000000   \n",
      "99           TWLOHA  18:16:00       en  1659990000000000000   \n",
      "\n",
      "                                                Tweet  location  \\\n",
      "0   [canst, even, express, angry, found, everythin...         0   \n",
      "1                   [sadness, anger, one, didn, vote]         0   \n",
      "2   [pale, geoffrey, love, like, share, condolence...         0   \n",
      "3           [ahahah, story, life, sadness, sex, life]         0   \n",
      "4   [never, heard, store, now, course, APSAP, stoc...         0   \n",
      "..                                                ...       ...   \n",
      "95  [nahjust, pitch, black, eternity, side, dish, ...         0   \n",
      "96  [really, anna, explore, feeling, loneliness, l...         0   \n",
      "97  [hello, podcast, thought, like, listen, conten...         0   \n",
      "98  [aged, anyone, can, volunteer, million, volunt...         0   \n",
      "99  [maybe, can, reach, someone, love, trust, ask,...         0   \n",
      "\n",
      "                Hashtags           Segmented  Sentiment  Diagnose   TimeSin  \\\n",
      "0                     []                 NaN    -0.5557         1  0.500000   \n",
      "1                     []                 NaN    -0.7650         1  0.866025   \n",
      "2                     []                 NaN     0.9451         0  0.965926   \n",
      "3                     []                 NaN     0.1779         0  0.965926   \n",
      "4                     []                 NaN     0.0000         1  0.965926   \n",
      "..                   ...                 ...        ...       ...       ...   \n",
      "95                    []                 NaN    -0.4215         1  0.866025   \n",
      "96                    []                 NaN    -0.5859         1 -0.258819   \n",
      "97  ['wellbeingmatters']  well being matters     0.8271         0 -0.707107   \n",
      "98                    []                 NaN     0.5106         0 -0.707107   \n",
      "99                    []                 NaN     0.8343         0 -1.000000   \n",
      "\n",
      "         TimeCos  \n",
      "0  -8.660254e-01  \n",
      "1  -5.000000e-01  \n",
      "2  -2.588190e-01  \n",
      "3  -2.588190e-01  \n",
      "4  -2.588190e-01  \n",
      "..           ...  \n",
      "95  5.000000e-01  \n",
      "96  9.659258e-01  \n",
      "97  7.071068e-01  \n",
      "98  7.071068e-01  \n",
      "99 -1.836970e-16  \n",
      "\n",
      "[100 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "pt['TimeSin'] = np.sin(2 * np.pi * pd.to_datetime(pt['Time'],format='%H:%M:%S').dt.hour / 24)\n",
    "pt['TimeCos'] = np.cos(2 * np.pi * pd.to_datetime(pt['Time'],format='%H:%M:%S').dt.hour / 24)\n",
    "\n",
    "le = LabelEncoder()\n",
    "pt[\"location\"]=le.fit_transform(pt[\"location\"])\n",
    "print(pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c5ec994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "canst even express angry found everything taught australian history lie found sadness felt felt denied land family also history just grew immensely \n",
      "sadness anger one didn vote \n",
      "pale geoffrey love like share condolence remember geoffrey fondness geoffrey active liked member HousingFirst community sadness share farewell recent passing \n",
      "ahahah story life sadness sex life \n",
      "never heard store now course APSAP stockSadness \n",
      "ah seeing result sadness comment wish change answer \n",
      "daughter loved photo need puppy deal sadness see work public hospital psychiatric ward clinic \n",
      "feel two different way one time possible anyone depression fattest many u experience blissful moment happiness never losing awareness deep blue sadness lurking \n",
      "toe sadness \n",
      "livid little dermoid nina turner Netflix k TikTok couch unbearable sadness succession ending licorice allsorts pop culture chat show pm along fellow culture grab join u \n",
      "went uncontrollable sobbing total mental paralysis passing nine year ago now minute sadness thought arise torture survive bury lover best memory loved pet deep heart \n",
      "whereas light without darkness good without bad happiness without sadness rich people without poor people \n",
      "badly netflix sadness \n",
      "marie DreamsBlank SpaceFake SmileAlejandroWide AwakeRemedyPrisonerDaddy LessonsHands MyselfNo love AllowedEvery timeSummertime sadness \n",
      "sadness \n",
      "moment lifestyle exilethank u nextTelephoneI kissed GirlSend love new see AgainBREAK SOULFourFiveSeconds hut DriveGimme EverytimeSummertime sadness radio \n",
      "please ensure check propos editing releasing damaging just audience cast well mentally draining due fear sadness many case trauma well human aid viewer go \n",
      "well sunday either happy tear hopefully river sadness tear \n",
      "word can see sadness eye \n",
      "dear god thank everything happiness sadness \n",
      "dont know true sadness haven seen film \n",
      "can believe go work tomorrow deal people d rather continue allow sadness darkness bedroom \n",
      "feeling sadness feeling joy \n",
      "now manages get across beauty moment deep disappointment sadness truly one grant gift well hope come back screen come back accepting world \n",
      "seems way can believe people saying good income today money failed see sadness \n",
      "fancy Nancys sadness \n",
      "powerful image theme often feel tinge sadness family move apartment across kind get know way coming going ve never actually met teres balcony window last christmas gone now \n",
      "feel great sadness prince carry hope doesn anything silly final act revenge royal family something ominous troubled soul spending time alone hotel room \n",
      "yes sadness \n",
      "one think dazzling bunch writer journalist sad doesn get play dinosaur daily basis striking anger sadness \n",
      "think right first year worst remember sad beloved area first almost yes track almost always remember without sadness hopefully case u future \n",
      "listened feel disgusted subjected appalling racism thought spoke great dignity sadness \n",
      "serving time political division day supposed bring people commonwealth together outrage king charles III spoken sadness way original men abuse woman child alarming rate \n",
      "will forever ponder relationship human maybe much mind go fear grief sadness shame happiness contentment desire complex beauty \n",
      "sound wonderfully fascinating sadness think enough life traced book \n",
      "Congrats denver well deserved also thanks Sportsbet compensation sadness \n",
      "obviously time just patience hope get help sadness fastness healthy \n",
      "realised something let go can seen c positively remembered valued first placeWhen show joyful even heart sad people beam positively back creates loopThe cure sadness making people happy \n",
      "dont understand sadness either west \n",
      "gone fast sadness excitement proud rep \n",
      "try destroy noble people dont always agree brought tear felt sadness shame dam good now grant stop shouting take responsibility past move future \n",
      "now look forward sadness watching Ahsoka moon see cast series absolutely dizzy pale may stevenson \n",
      "incredible sorry can imagine sadness x x chi chi love much \n",
      "sadness really misplaced \n",
      "just want happy ll know can bad day go period sadness whatever feel like just existing ll just want happy consistently \n",
      "happy anniversary Marianna amid sadness many happy memory sang \n",
      "definitely easy letting go okay sadness found meditation helped area understand wound like well good say small step make coffee please LOL \n",
      "working letting grief cripple nothing say think will change situation need ok feeling sadness let make negative life embrace let go lost wont easy \n",
      "point portrait sadness brush stroke melancholy hue sorrow maudlin masterpiece hang museum heart \n",
      "girlfriend experience isn just selling lid picsUnderstand user want moreThey re seeking remedy lonelinessSomething canst get real life free tube sitesContent alone isn enough lack right context \n",
      "now loneliness imparts brainLoneliness bad health smoking cigarette daily scientist warned previously based many year research \n",
      "luck loneliness hurt keeping brave face becoming increasingly difficultWish loyal friend confide inWish cut boy girl muddle spend every day though problem likely tucked rear BPD mental issue \n",
      "frequent user YouTube higher level loneliness anxiety depression according research australian institute suicide research prevention AISRAP \n",
      "died loneliness gotten phone plea maybe kept entertained nephew died killed shotgun blaming politician though even lack training purpose responsibility gun pushed hand fear based marketing blame brother law store purchased army local range never went girlfriend broke people killed gun themselvesNot saying mon killed Trumps fault ever go jail will never \n",
      "poor matas follow lot rescue instagram kill much suffering loneliness animal endure overseas never see stray animal \n",
      "maaaaaaaaannn something vent respected people gun talk even ground loneliness anyone stand side confide LIKE helplessness can isolation hard describe \n",
      "holding mirror overaccomodation reduces engagement increase loneliness \n",
      "loneliness \n",
      "people recreation relieve stress loneliness etc \n",
      "june mental health update scared night feeling somewhat scared feeling experiencing loneliness safety ensure feeling loved hating body probably cold normal good way drinking cope \n",
      "picture called loneliness girl park gave just finished drawing itWas time beautiful captivating \n",
      "LONELINESSEATING INSIDEITS BURNING ALIVEIM TRAVELING MINDIM FLYING THROUGH THOUSAND LONELY SOULSBATTLE CRYIN SWAMPS EVIL LIESWERE CLOSER THE TRUTH NOWTAKE HOME \n",
      "think someone know might lonely share online directory help find connect activity people \n",
      "feeling loneliness normal part human experience time can confronting someone know feeling lonely tip sheet may help \n",
      "hunt long list research exploring YOUNG men across cross academic NGOs anything alcohol gazing loneliness want lay land know sector bag mate \n",
      "loneliness chair \n",
      "surgeon general report public health epidemic loneliness can reduced society lack three thing \n",
      "loss loneliness something never prepared year old supposed time dating travelling making sidelong memory instead bedridden manage drag fear letting pp \n",
      "can sign dementia sign desperate loneliness working elderly can challenging yet incredibly rewardingIts hard try talking can give feedback carersOr may just nasty also exist \n",
      "though anna bookclub Michelle manner trying MartLaura McPheeBrowne cherry BeachEllen van Neerven personal ScoreRadclyffe well loneliness cruel CareAndre Indelible city \n",
      "intelligent man good open loneliness complex one big factor cost housing working longer le income rent mortgage helping bring k emigrant rental market already crisis \n",
      "RePost loneliness \n",
      "nature power combat loneliness isolation vulnerable group \n",
      "alone divided wont consumed hate undecided allowing loneliness seal fate love spirit provided well see worth wait \n",
      "look forward working federal government improve social connection address australia \n",
      "today BidenHarris administration announced promoting will priority \n",
      "absolutely struggle loneliness social medium connection help lot \n",
      "controlling much easy access self owned technology transport sore many problem eg loneliness epidemic cause depression leading crime death health work absentee etc doesn mean technology car just public shared facility \n",
      "one loneliness crowd \n",
      "periodically see people suggesting root solution loneliness isolation old people marvel easily get drunk technology miss human point thing \n",
      "authentic word concept get thrown around connect authenticity hate dismissed authentic authentic connection antidote mean \n",
      "initiative like hatty safe australia important helping prevent loneliness connecting people community \n",
      "SBSs alone illustrates positive psychology concept like mental roughness grit combating loneliness goalsetting awe pride survive alone wilderness unknown length time \n",
      "yesterday sometimes feel nothing loneliness tough \n",
      "absolute pleasure wrist timely one tune light workforce welling endeavour sydney loneliness epidemic requires u even mindful welling colleague work isolation remote \n",
      "health risk LonelinessSocial isolation associated increased risk dementia cause poor behaviour loneliness \n",
      "now loneliness epidemic affect mental health head blow loneliness inperson connection TWLOHAs editor lecky best \n",
      "loneliness toxic effect train activity may explain condition \n",
      "within advisory strategy address loneliness epidemic six pillar last one stand cultivate culture connection something can start right now without funding promise action politician \n",
      "st lockdown started making contact visitor thou drug use always able generate considered social hallucination started lot due boredom lonelinessthen took life tried get join \n",
      "barmaid merrier terrible aching loneliness loss emigrant \n",
      "powerful realization sometimes embracing solitude rousing personal growth can bring deep sense peace contentment reminder true fulfillment come within selfimprovement can alleviate feeling loneliness \n",
      "feeling emptiness feeling loneliness swarm dont know move past just want hold \n",
      "ex went shop shop people knew putting ulcer telling many embarrassing lie gang talking friend impact socially church utterly alone afraid even go shop worked bring loneliness now may alone happy stopped \n",
      "glad independent learnt happy cheerful despite loneliness material difficulty dont want live like forever either better help \n",
      "nahjust pitch black eternity side dish loneliness \n",
      "really anna explore feeling loneliness lonely hat current \n",
      "hello podcast thought like listen content episode go heart welling walk concept become passion project role play confronting loneliness epidemic \n",
      "aged anyone can volunteer million volunteer across australia play key part addressing social isolation loneliness older people living residential aged care facility receiving care home \n",
      "maybe can reach someone love trust ask spend quality time together possible enjoy time intentionally sans distraction see feel afterward fulfilling feeling loneliness ebb bit \n",
      "0     canst even express angry found everything taug...\n",
      "1                          sadness anger one didn vote \n",
      "2     pale geoffrey love like share condolence remem...\n",
      "3                   ahahah story life sadness sex life \n",
      "4      never heard store now course APSAP stockSadness \n",
      "                            ...                        \n",
      "95    nahjust pitch black eternity side dish lonelin...\n",
      "96    really anna explore feeling loneliness lonely ...\n",
      "97    hello podcast thought like listen content epis...\n",
      "98    aged anyone can volunteer million volunteer ac...\n",
      "99    maybe can reach someone love trust ask spend q...\n",
      "Name: Tweet, Length: 100, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#vectorization of the tokenized tweets entries\n",
    "def joinop(x):\n",
    "    s = \"\"\n",
    "    for i in x:\n",
    "        s = s + i + \" \";\n",
    "    print(s)\n",
    "    return s\n",
    "pt['Tweet'] = pt['Tweet'].apply(lambda x:  joinop(x))\n",
    "print(pt['Tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b46ed3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pt.iloc[:,4].copy()\n",
    "R = X\n",
    "target_label = pt.iloc[:,9]\n",
    "\n",
    "#ngram_range = (1,2)\n",
    "tfidf = TfidfVectorizer(sublinear_tf = True, max_df = 0.5,min_df = 0.001, stop_words='english',ngram_range = (1,2))\n",
    "X = tfidf.fit_transform(X)\n",
    "\n",
    "location_feature_array = pt.iloc[:,5].values.reshape(-1, 1)\n",
    "time_feature_array_sin = pt.iloc[:,10].values.reshape(-1, 1)\n",
    "time_feature_array_cos = pt.iloc[:,11].values.reshape(-1, 1)\n",
    "text_feature_dense = X.toarray()\n",
    "\n",
    "all_features = np.concatenate((text_feature_dense, location_feature_array, time_feature_array_sin, time_feature_array_cos), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46f3e873",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(all_features, target_label, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "00c30db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 2025)\n",
      "(100, 2022)\n"
     ]
    }
   ],
   "source": [
    "print(all_features.shape)\n",
    "print(text_feature_dense.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7c0f2995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 2013)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "X_train = X_train[:,:-3]\n",
    "X_test = X_test[:,:-3]\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train,y_train)\n",
    "y_pred1 = model.predict(X_test)\n",
    "tt = metrics.accuracy_score(y_test,y_pred1)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "372fd5ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "ans = 0\n",
    "for i,j in zip(y_pred,y_pred1):\n",
    "    if i == j :\n",
    "        ans= ans + 1\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b341a9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdsha\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Accuracy :  0.58\n",
      "Precision :  0.65625\n",
      "Sensitivity_recall :  0.6774193548387096\n",
      "Specificity :  0.42105263157894735\n",
      "F1_score :  0.6666666666666667\n"
     ]
    }
   ],
   "source": [
    "xgb_m = XGBClassifier(use_label_encoder=False, \n",
    "                      booster='gbtree', # boosting algorithm to use, default gbtree, othera: gblinear, dart\n",
    "                      n_estimators=100, # number of trees, default = 100\n",
    "                      eta=0.3, # this is learning rate, default = 0.3\n",
    "                      max_depth=6, # maximum depth of the tree, default = 6\n",
    "                      gamma = 0, # used for pruning, if gain < gamma the branch will be pruned, default = 0\n",
    "                      reg_lambda = 1, # regularization parameter, defautl = 1\n",
    "                      #min_child_weight=0 # this refers to Cover which is also responsible for pruning if not set to 0\n",
    "                     )\n",
    "xgb_m.fit(X_train, y_train)\n",
    "\n",
    "y_pred = xgb_m.predict(X_test)\n",
    "\n",
    "Accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "Precision = metrics.precision_score(y_test, y_pred)\n",
    "Sensitivity_recall = metrics.recall_score(y_test, y_pred)\n",
    "Specificity = metrics.recall_score(y_test, y_pred, pos_label=0)\n",
    "F1_score = metrics.f1_score(y_test, y_pred)\n",
    "print(\"\\n\")\n",
    "print(\"Accuracy : \",Accuracy)\n",
    "print(\"Precision : \",Precision)\n",
    "print(\"Sensitivity_recall : \",Sensitivity_recall)\n",
    "print(\"Specificity : \",Specificity)\n",
    "print(\"F1_score : \",F1_score)\n",
    "\n",
    "\n",
    "# # define metrics\n",
    "# y_pred_proba = model.predict_proba(test)[::,1]\n",
    "# fpr, tpr, _ = metrics.roc_curve(ytest,  y_pred_proba)\n",
    "# auc = metrics.roc_auc_score(ytest, y_pred_proba)\n",
    "\n",
    "# #create ROC curve\n",
    "# plt.plot(fpr,tpr,label=\"AUC=\"+str(auc))\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.legend(loc=4)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2944b301",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Accuracy :  0.64\n",
      "Precision :  0.6585365853658537\n",
      "Sensitivity_recall :  0.8709677419354839\n",
      "Specificity :  0.2631578947368421\n",
      "F1_score :  0.75\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "clf = lgb.LGBMClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "# LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
    "#                importance_type='split', learning_rate=0.1, max_depth=-1,\n",
    "#                min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
    "#                n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\n",
    "#                random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
    "#                subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\n",
    "\n",
    "y_pred0 =clf.predict(X_test)\n",
    "Accuracy = metrics.accuracy_score(y_test, y_pred0)\n",
    "Precision = metrics.precision_score(y_test, y_pred0)\n",
    "Sensitivity_recall = metrics.recall_score(y_test, y_pred0)\n",
    "Specificity = metrics.recall_score(y_test, y_pred0, pos_label=0)\n",
    "F1_score = metrics.f1_score(y_test, y_pred0)\n",
    "print(\"\\n\")\n",
    "print(\"Accuracy : \",Accuracy)\n",
    "print(\"Precision : \",Precision)\n",
    "print(\"Sensitivity_recall : \",Sensitivity_recall)\n",
    "print(\"Specificity : \",Specificity)\n",
    "print(\"F1_score : \",F1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7829f24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import is_classifier\n",
    "\n",
    "print(is_classifier(xgb_m))\n",
    "print(is_classifier(clf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d4bf112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(C=1, gamma=2)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=1, gamma=2)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(C=1, gamma=2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=10) # Define classifier\n",
    "rf.fit(X_train, y_train) # Train model\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth=5) # Define classifier\n",
    "dt.fit(X_train, y_train) # Train model\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_rbf = SVC(gamma=2, C=1)\n",
    "svm_rbf.fit(X_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7e9f214b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdsha\\anaconda3\\lib\\site-packages\\mlxtend\\classifier\\ensemble_vote.py:172: UserWarning: fit_base_estimators=False enforces use_clones to be `False`\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Feature shape mismatch, expected: 2016, got 2013",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16748/1918888811.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0meclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEnsembleVoteClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclfs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mxgb_m\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msvm_rbf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_base_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0meclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m# Build stack model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\mlxtend\\classifier\\ensemble_vote.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# 'hard' voting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 231\u001b[1;33m             \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m             maj = np.apply_along_axis(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\mlxtend\\classifier\\ensemble_vote.py\u001b[0m in \u001b[0;36m_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    308\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m             return np.asarray(\n\u001b[1;32m--> 310\u001b[1;33m                 \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mle_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclfs_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    311\u001b[0m             ).T\n\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\mlxtend\\classifier\\ensemble_vote.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    308\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m             return np.asarray(\n\u001b[1;32m--> 310\u001b[1;33m                 \u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mle_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclfs_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    311\u001b[0m             ).T\n\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X, output_margin, ntree_limit, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[0;32m   1523\u001b[0m     ) -> np.ndarray:\n\u001b[0;32m   1524\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mverbosity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbosity\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1525\u001b[1;33m             class_probs = super().predict(\n\u001b[0m\u001b[0;32m   1526\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1527\u001b[0m                 \u001b[0moutput_margin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_margin\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X, output_margin, ntree_limit, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[0;32m   1112\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_use_inplace_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m                     predts = self.get_booster().inplace_predict(\n\u001b[0m\u001b[0;32m   1115\u001b[0m                         \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m                         \u001b[0miteration_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0miteration_range\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36minplace_predict\u001b[1;34m(self, data, iteration_range, predict_type, missing, validate_features, base_margin, strict_shape)\u001b[0m\n\u001b[0;32m   2267\u001b[0m                 )\n\u001b[0;32m   2268\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2269\u001b[1;33m                 raise ValueError(\n\u001b[0m\u001b[0;32m   2270\u001b[0m                     \u001b[1;34mf\"Feature shape mismatch, expected: {self.num_features()}, \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2271\u001b[0m                     \u001b[1;34mf\"got {data.shape[1]}\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Feature shape mismatch, expected: 2016, got 2013"
     ]
    }
   ],
   "source": [
    "# from sklearn.ensemble import StackingClassifier\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# estimator_list = [\n",
    "#     ('xgb_m',xgb_m),\n",
    "#     ('svm_rbf',svm_rbf),\n",
    "#     ('dt',dt),\n",
    "#     ('rf',rf),\n",
    "#     ('lightgbm',clf) \n",
    "    \n",
    "from mlxtend.classifier import EnsembleVoteClassifier\n",
    "\n",
    "eclf = EnsembleVoteClassifier(clfs=[xgb_m, svm_rbf, dt,rf,clf], weights=[1,1,1,1,1], fit_base_estimators=False)\n",
    "eclf.fit(X_train,y_train)\n",
    "y_pred = eclf.predict(X_test)\n",
    "print(y_pred)\n",
    "# Build stack model\n",
    "# stack_model = StackingClassifier(\n",
    "#     estimators=estimator_list, final_estimator=LogisticRegression()\n",
    "# )\n",
    "\n",
    "# # Train stacked model\n",
    "# stack_model.fit(X_train, y_train)\n",
    "\n",
    "# # Make predictions\n",
    "# y_train_pred = stack_model.predict(X_train)\n",
    "# y_test_pred = stack_model.predict(X_test)\n",
    "\n",
    "# # Training set model performance\n",
    "# stack_model_train_accuracy = accuracy_score(y_train, y_train_pred) # Calculate Accuracy\n",
    "\n",
    "\n",
    "# # Test set model performance\n",
    "# stack_model_test_accuracy = accuracy_score(y_test, y_test_pred) # Calculate Accuracy\n",
    "\n",
    "\n",
    "# print('Model performance for Training set')\n",
    "# print('- Accuracy: %s' % stack_model_train_accuracy)\n",
    "\n",
    "# print('----------------------------------')\n",
    "# print('Model performance for Test set')\n",
    "# print('- Accuracy: %s' % stack_model_test_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "490530c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mdsha\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
      "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "predict_proba is not available when  probability=False",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16748/3785747489.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# Make predictions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvoting_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mprecision\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprecision_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_voting.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    361\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvoting\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"soft\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m             \u001b[0mmaj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# 'hard' voting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_voting.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    402\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m         avg = np.average(\n\u001b[1;32m--> 404\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_collect_probas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_weights_not_none\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    405\u001b[0m         )\n\u001b[0;32m    406\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mavg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_voting.py\u001b[0m in \u001b[0;36m_collect_probas\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    377\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_collect_probas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m         \u001b[1;34m\"\"\"Collect results from clf.predict calls.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 379\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    380\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_voting\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_voting.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    377\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_collect_probas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m         \u001b[1;34m\"\"\"Collect results from clf.predict calls.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 379\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    380\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_voting\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_available_if.py\u001b[0m in \u001b[0;36m__get__\u001b[1;34m(self, obj, owner)\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[1;31m# delegate only on instances, not the classes.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[1;31m# this is to allow access to the docstrings.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mattr_err\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMethodType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36m_check_proba\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    828\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobability\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 829\u001b[1;33m             raise AttributeError(\n\u001b[0m\u001b[0;32m    830\u001b[0m                 \u001b[1;34m\"predict_proba is not available when  probability=False\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    831\u001b[0m             )\n",
      "\u001b[1;31mAttributeError\u001b[0m: predict_proba is not available when  probability=False"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "# Define the VotingClassifier\n",
    "voting_model = VotingClassifier(\n",
    "    estimators=[('xgb_m',xgb_m),('svm_rbf',svm_rbf),('dt',dt),('rf',rf),('lightgbm',clf) ],voting='soft')\n",
    "\n",
    "# Train the VotingClassifier\n",
    "voting_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = voting_model.predict(X_test)\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "precision = metrics.precision_score(y_test, y_pred)\n",
    "recall = metrics.recall_score(y_test, y_pred)\n",
    "f1_score = metrics.f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1-score:\", f1_score)\n",
    "\n",
    "# from sklearn.ensemble import StackingClassifier\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# estimator_list = [\n",
    "#     ('clf',clf),\n",
    "#     ('model',model)]\n",
    "\n",
    "# # Build stack model\n",
    "# stack_model = StackingClassifier(\n",
    "#     estimators=estimator_list, final_estimator=LogisticRegression()\n",
    "# )\n",
    "\n",
    "# # Train stacked model\n",
    "# stack_model.fit(X_train, y_train)\n",
    "\n",
    "# # Make predictions\n",
    "# y_train_pred = stack_model.predict(X_train)\n",
    "# y_test_pred = stack_model.predict(X_test)\n",
    "\n",
    "# # Training set model performance\n",
    "# stack_model_train_accuracy = accuracy_score(y_train, y_train_pred) # Calculate Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9ccf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_mat = confusion_matrix(y_test, y_pred0)\n",
    "print(confusion_mat)\n",
    "print(metrics.accuracy_score(y_train, xgb_m.predict(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5063bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_reshaped = np.reshape(y_pred, (-1, 1))\n",
    "# X_test = np.concatenate((X_test,y_pred_reshaped), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38f924f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, Embedding, Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from keras.layers import Input, Dense, LSTM, Conv1D, Dropout, Bidirectional, Multiply\n",
    "\n",
    "def attention(inputs):\n",
    "    \"\"\"\n",
    "    Custom attention layer to compute attention weights and apply them to the LSTM outputs.\n",
    "    \"\"\"\n",
    "    attention_units = 64  # Number of units in the attention layer\n",
    "    \n",
    "    hidden_states = inputs[0]\n",
    "    hidden_size = int(hidden_states.shape[2])\n",
    "    \n",
    "    # Compute attention scores\n",
    "    attention_scores = Dense(attention_units, activation='tanh')(hidden_states)\n",
    "    attention_scores = Dense(1, activation='softmax')(attention_scores)\n",
    "    \n",
    "    # Apply attention scores to LSTM outputs\n",
    "    weighted_hidden_states = tf.multiply(hidden_states, attention_scores)\n",
    "    \n",
    "    # Sum the weighted LSTM outputs\n",
    "    context_vector = tf.reduce_sum(weighted_hidden_states, axis=1)\n",
    "    \n",
    "    return context_vector\n",
    "\n",
    "def create_attention_lstm_model(input_shape, num_classes):\n",
    "    \"\"\"\n",
    "    Create an Attention-LSTM model for depression detection.\n",
    "    \"\"\"\n",
    "#     inputs = Input(shape=(20, input_shape[1]))\n",
    "#     lstm_units = 128\n",
    "#     x = Conv1D(filters=64, kernel_size=1, activation='relu')(inputs)  # padding = 'same'\n",
    "#     x = Dropout(0.3)(x)\n",
    "\n",
    "#     # lstm_out = Bidirectional(LSTM(lstm_units, activation='relu'), name='bilstm')(x)\n",
    "#     lstm_out = Bidirectional(LSTM(lstm_units, return_sequences=True))(x)\n",
    "#     lstm_out = Dropout(0.3)(lstm_out)\n",
    "#     #attention_mul = Flatten()(lstm_out)\n",
    "\n",
    "#     output = Dense(1, activation='sigmoid')(lstm_out)\n",
    "#     model = Model(inputs=[inputs], outputs=output)\n",
    "#     return model\n",
    "    lstm_units = 128  # Number of units in the LSTM layer\n",
    "    dense_units = 64  # Number of units in the dense layer\n",
    "    dropout_rate = 0.2  # Dropout rate\n",
    "    \n",
    "    # Input layer\n",
    "    inputs = Input(shape=(20, input_shape[1]))\n",
    "    # LSTM layer\n",
    "    lstm_outputs = Bidirectional(LSTM(lstm_units, activation='relu'), name='bilstm')(inputs)\n",
    "    \n",
    "    # Apply attention mechanism\n",
    "    context_vector = attention(lstm_outputs)\n",
    "    \n",
    "    # Dropout layer\n",
    "    dropout = Dropout(dropout_rate)(context_vector)\n",
    "    \n",
    "    # Dense layer\n",
    "    dense = Dense(dense_units, activation='relu')(dropout)\n",
    "    \n",
    "    # Output layer\n",
    "    outputs = Dense(num_classes, activation='sigmoid')(dense)\n",
    "    \n",
    "    # Create the model\n",
    "    model = Model(inputs=[inputs], outputs=outputs)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Example usage\n",
    "input_shape = X_test.shape  # Define the input shape of your data\n",
    "num_classes = 1  # Number of classes (1 for binary classification)\n",
    "model = create_attention_lstm_model(input_shape, num_classes)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_test, y_test, batch_size=32, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c23d11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_train, y_train)\n",
    "print('Test Loss:', loss)\n",
    "print('Test Accuracy:', accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
